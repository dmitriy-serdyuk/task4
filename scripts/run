#!/usr/bin/env python
from __future__ import print_function

import numpy as np

import torch
from argparse import ArgumentParser
from torch import nn
from torch.utils.data import DataLoader

from attend_to_detect.model.stft_dataset import STFTDataset
from attend_to_detect.model.model import (
    AttendToDetect, train_fn, valid_fn, cost)
from attend_to_detect.utils.pytorch_utils import TrainLoop

from fuel.datasets.hdf5 import H5PYDataset
from fuel.streams import DataStream
from fuel.schemes import (SequentialScheme, ShuffledScheme)
from fuel.transformers import Mapping, ScaleAndShift


def padder(data):
    data = list(data)
    for index in [0, -2, -1]:
        max_ts = np.max([datum.shape[-2] for datum in data[index]])

        for i in range(len(data[index])):
            len_dif = max_ts - data[index][i].shape[-2]
            if len_dif > 0:
                data[index][i] = np.concatenate((
                    data[index][i],
                    np.zeros((1, len_dif, data[index][i].shape[-1]))),
                    axis=-2
                )
                if index != 0:
                    data[index][i][:, -len_dif:, 0] = 1
    data = tuple(data)

    return data


def load_data(batch_size, feature_mean=0.0, feature_std=1.0,
              path='/data/lisatmp4/santosjf/task4/attend_to_detect/create_dataset/dcase_2017_task_4_test.hdf5'):
    sources = ['audio_features', 'targets_weak_alarm', 'targets_weak_vehicle']
    train_data = H5PYDataset(path, which_sets=('train',), sources=sources)
    valid_data = H5PYDataset(path, which_sets=('valid',), sources=sources)
    train_scheme = ShuffledScheme(train_data.num_examples, batch_size=batch_size)
    valid_scheme = SequentialScheme(valid_data.num_examples, batch_size=batch_size)
    train_stream = DataStream(dataset=train_data, iteration_scheme=train_scheme)
    valid_stream = DataStream(dataset=valid_data, iteration_scheme=valid_scheme)

    # Add streams to pad/normalize data here
    train_stream = ScaleAndShift(
        Mapping(
        train_stream,
            mapping=padder
        ), -feature_mean/feature_std,
        1/feature_std,
        sources=('audio_features',)
    )
    valid_stream = ScaleAndShift(
        Mapping(
            valid_stream,
            mapping=padder
        ), -feature_mean/feature_std, 1/feature_std,
        sources=('audio_features', )
    )

    return train_stream, valid_stream


def main(window_size, step_size, train_file, valid_file, checkpoint_path, max_epochs):
    np.random.seed(args.seed)  # for reproducibility

    input_dim = (1,)
    decoder_dim = (1,)
    output_classes = 10

    model = AttendToDetect(input_dim, decoder_dim, output_classes)
    criterion = cost
    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)

    G_train, G_val = load_data(window_size, step_size, train_file=train_file, valid_file=valid_file)
    train_loader = DataLoader(G_train, batch_size=args.batch_size,
                              num_workers=4, shuffle=True)
    valid_loader = DataLoader(G_val, batch_size=args.batch_size,
                              num_workers=2)

    train_loop = TrainLoop(model,
                           optimizer, criterion,
                           train_fn, train_loader,
                           valid_fn, valid_loader,
                           checkpoint_path=checkpoint_path)

    try:
        train_loop.load_checkpoint(checkpoint_path)
        print('Loading checkpoint from {}'.format(checkpoint_path))
    except ValueError:
        print('No checkpoints, initializing a model from scratch...')

    train_loop.train(max_epochs)


if __name__ == '__main__':
    parser = ArgumentParser()

    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--max_epochs', type=int, default=100)
    parser.add_argument('--learning_rate', type=float, default=1e-3)
    parser.add_argument('--seed', default=42)
    parser.add_argument('checkpoint_path')

    args = parser.parse_args()

    main(window_size, step_size, args.train_file, args.valid_file, args.checkpoint_path, args.max_epochs)

# EOF
