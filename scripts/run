#!/usr/bin/env python
from __future__ import print_function

import numpy as np

import torch
from argparse import ArgumentParser
from torch import nn
from torch.utils.data import DataLoader

from attend_to_detect.model.stft_dataset import STFTDataset
from attend_to_detect.model.model import (
    AttendToDetect, train_fn, valid_fn, cost)
from attend_to_detect.utils.pytorch_utils import TrainLoop

from fuel.datasets.hdf5 import H5PYDataset
from fuel.streams import DataStream
from fuel.schemes import (SequentialScheme, ShuffledScheme)
from fuel.transformers import ScaleAndShift, Cast, Padding

def load_data(batch_size, feature_mean=0.0, feature_std=1.0,
        path='/data/lisatmp4/santosjf/task4/attend_to_detect/create_dataset/dcase_2017_task_4_test.hdf5'):
    sources = ['audio_features', 'targets_weak_alarm', 'targets_weak_vehicle']
    train_data = H5PYDataset(path, which_sets=('train',), sources=sources)
    valid_data = H5PYDataset(path, which_sets=('valid',), sources=sources)
    train_scheme = ShuffledScheme(train_data.num_examples, batch_size=batch_size)
    valid_scheme = SequentialScheme(valid_data.num_examples, batch_size=batch_size)
    train_stream = DataStream(dataset=train_data, iteration_scheme=train_scheme)
    valid_stream = DataStream(dataset=valid_data, iteration_scheme=valid_scheme)

    # Add streams to pad/normalize data here
    train_stream = ScaleAndShift(Padding(train_stream, mask_sources=('targets_weak_alarm', 'targets_weak_vehicle')),
        -feature_mean/feature_std, 1/feature_std, sources=('audio_features',))
    valid_stream = ScaleAndShift(Padding(valid_stream, mask_sources=('targets_weak_alarm', 'targets_weak_vehicle')),
        -feature_mean/feature_std, 1/feature_std, sources=('audio_features',))


    return train_stream, valid_stream


if __name__ == '__main__':
    parser = ArgumentParser()

    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--max_epochs', type=int, default=100)
    parser.add_argument('--learning_rate', type=float, default=1e-3)
    parser.add_argument('--seed', default=42)
    parser.add_argument('checkpoint_path')

    args = parser.parse_args()
    np.random.seed(args.seed)  # for reproducibility

    input_dim = (1,)
    decoder_dim = (1,)
    output_classes = 10

    model = AttendToDetect(input_dim, decoder_dim, output_classes)
    criterion = cost
    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)

    G_train, G_val = load_data(window_size, step_size, train_file=args.train_file, valid_file=args.valid_file)
    train_loader = DataLoader(G_train, batch_size=args.batch_size,
                              num_workers=4, shuffle=True)
    valid_loader = DataLoader(G_val, batch_size=args.batch_size,
                              num_workers=2)

    train_loop = TrainLoop(model,
                           optimizer, criterion,
                           train_fn, train_loader,
                           valid_fn, valid_loader,
                           checkpoint_path=args.checkpoint_path)

    try:
        train_loop.load_checkpoint(args.checkpoint_path)
        print('Loading checkpoint from {}'.format(checkpoint_path))
    except ValueError:
        print('No checkpoints, initializing a model from scratch...')

    train_loop.train(args.max_epochs)
